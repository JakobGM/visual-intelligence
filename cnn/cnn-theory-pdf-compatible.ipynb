{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Classification and CNN\n",
    "\n",
    "_Authors: Martin Outzen Berild & Jakob Gerhard Martinussen_\n",
    "\n",
    "This presentation is better viewed at the following url: [http://folk.ntnu.no/jakobgm/cnn-theory.slides.html#/](http://folk.ntnu.no/jakobgm/cnn-theory.slides.html#/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/image_classification.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image classification before deep learning\n",
    "\n",
    "1. Feature extraction:\n",
    "    * **Naive:** Flatten $n \\times m$ RGB image into $\\mathcal{R}^{3nm}$ vector.\n",
    "    * **Manual feature extraction:** Custom feature extraction implemented by programmer based on domain-knowledge.\n",
    "2. Implement model based on features:\n",
    "    * **Explicit algorithm:** Pattern matching, etc.\n",
    "    * **Statistical model:** Support vector machine (SVM), etc.\n",
    "    \n",
    "Drawbacks:\n",
    "* Non-robust.\n",
    "* Time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep learning approach to image classification\n",
    "\n",
    "![](img/deep_learning_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Convolutional Neural Networks Theory (ConvNets)\n",
    "\n",
    "_Keywords: receptive field, input, kernel, zero-padding, strides, output._\n",
    "\n",
    "![](img/convolutional_networks.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors vs. Matrices\n",
    "\n",
    "**Key difference** - Inputs to CNNs are 2-D arrays instead of linearly indexed vectors.\n",
    "\n",
    "![](img/matrix_vs_vector.png)\n",
    "_Source: [jeremyjordan.me](https://www.jeremyjordan.me)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Receptive Fields\n",
    "\n",
    "_Resource: [A Guide to Receptive Field Arithmetic for Convolutional Neural Networks](https://syncedreview.com/2017/05/11/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks/)_\n",
    "\n",
    "<img src=\"img/receptive_fields.gif\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Number of filters to learn\n",
    "NUMBER_OF_FILTERS = 1\n",
    "\n",
    "# Size of receptive field\n",
    "KERNEL_SIZE = (3, 3)\n",
    "\n",
    "convolutional_layer = layers.Conv2D(\n",
    "    filters=NUMBER_OF_FILTERS,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "The process of moving the receptive field over the image, weighting each pixel in order to form new pixel values.\n",
    "\n",
    "This results in a so-called _\"feature map\"_, and can be thought of as a filtered image.\n",
    "\n",
    "<img src=\"img/convolution.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "* A single bias value is also included.\n",
    "* The value is passed through an activation function as well.\n",
    "* The same kernel weights and bias value is used over the entire image (_weight/parameter sharing_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kernel\n",
    "\n",
    "_A set of weights defined over the receptive field._\n",
    "\n",
    "![](img/kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/convolution_kernels_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/convolution_kernels_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stride\n",
    "\n",
    "_Step size of the receptive field as it moves over the input image._\n",
    "\n",
    "<img src=\"img/stride_1.gif\" width=\"49%\"> <img src=\"img/stride_2.gif\" width=\"49%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding\n",
    "\n",
    "<img src=\"img/padding.gif\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Convolution without padding and with stride of 1\n",
    "\n",
    "<img src=\"img/example_1.gif\" height=\"30%\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolution with zero-padding and with stride of 1\n",
    "\n",
    "<img src=\"img/example_2.gif\" height=\"30%\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolution with zero-padding and with stride of 2\n",
    "\n",
    "<img src=\"img/example_3.gif\" height=\"30%\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooling layer\n",
    "\n",
    "* A process which follows the convolution in CNNs.\n",
    "* Also known as _subsampling_.\n",
    "* Motivated by a model of the mammal visual cortex.\n",
    "\n",
    "> [...] a reduction in spatial resolution appears to be responsible for achieving translational invariance.\n",
    "\n",
    "* _Pooling_ is a way of modelling this reduction in dimensionality.\n",
    "* Benefits: makes the training computationally feasible and prevents overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Glossary\n",
    "\n",
    "* **pooled feature map** - the result of subsampling the convolutional feature map.\n",
    "* **pooling layer** - a collection of pooled feature maps.\n",
    "* **pooling neighborhood** - a subdivision of a feature map (usually 2x2) which is replaced by _one single_ value.\n",
    "* **adjacent pooling neighbourhood** - non-overlapping, side-by-side neighbourhoods.\n",
    "* **pooling method** - process of reducing neighbourhood down to one single value.\n",
    "    * **average pooling** - average of values within neighbourhood.\n",
    "    * **max pooling** - maximim value of neighbourhood.\n",
    "    * **$L_2$ pooling** - square root of sum of values in neighbourhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/maxpool.gif\" height=\"50%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "Classification is performed by a fully connected neural network (FCNN).\n",
    "\n",
    "* The FCNN's input comes from the last _pooled feature map_.\n",
    "* The 2D feature map is vectorized (read flattened) and fed into the input layer of the FCNN.\n",
    "\n",
    "<img src=\"img/cnn_overview.gif\" height=\"80%\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical Formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "* Let $w \\in \\mathcal{R}^{\\texttt{KERNEL HEIGHT}~\\times~\\texttt{KERNEL WIDTH}}$ denote a given kernel.\n",
    "* Let $a_{x, y} \\in \\mathcal{R}$ denote image or pooled feature pixel value at index $(x, y)$\n",
    "    * $x = 1, ..., \\texttt{LAYER HEIGHT}$.\n",
    "    * $y = 1, ..., \\texttt{LAYER WIDTH}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The _convolution operator_, $\\circledast$, is then defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    w \\circledast a_{x, y} = \\sum_{i} \\sum_{j} w_{i, j} ~ a_{x - i, y - j}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Adding a _bias_, $b$, we define a _weighted output_, $z$, of the convolution as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    z_{x,y} = w \\circledast a_{x,y} + b\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forward pass\n",
    "\n",
    "* Let $l = 1, ..., L_{c}$ index the $L_c$ convolutional layers in the architecture. We then have:\n",
    "\n",
    "\\begin{equation*}\n",
    "    z_{x,y}^{(l)} = w^{(l)} \\circledast a_{x,y}^{(l - 1)} + b^{(l)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Let $f: \\mathcal{R} \\rightarrow \\mathcal{R}$ be the _activation function_ of choice. We then have:\n",
    "\n",
    "\\begin{equation*}\n",
    "    a_{x,y}^{(l)} = f ~ \\left(z_{x,y}^{(l)}\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $a_{x,y}^{(0)} = \\{\\text{values of pixels in the original input image}\\}$.\n",
    "* $a_{x,y}^{(L_c)} = \\{\\text{values of pooled features in last layer of the CNN}\\}$.\n",
    "* NB! The notation is a bit sloppy here, as $a_{x,y}^{(l)}$ is the result of a downsampling (pooling) in addition to an activation $f(...)$. Consider $f(...)$ to be the activation function and downsampler function from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/kernel_convolution.gif\" height=\"60%\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Let $C$ denote the cost function.\n",
    "* The error at position $(x, y)$ in the _pooled layer_ number $l$ is then defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\delta_{x,y}^{(l)} = \\frac{\\partial C}{\\partial z_{x,y}^{(l)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $\\mathcal{I}$ and $\\mathcal{J}$ be index sets which contain all the indeces of $i$ and $j$ which are involved in the calculation of $z_{x, y}$. Using the chain rule we can relate $\\delta_{x,y}^{(l)}$ to $\\delta_{x,y}^{(l+1)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\delta_{x,y}^{(l)}\n",
    "    =\n",
    "    \\frac{\\partial C}{\\partial z_{x,y}^{(l)}}\n",
    "    =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\frac{\\partial C}{\\partial z_{i,j}^{(l+1)}} \\frac{\\partial z_{i,j}^{(l+1)}}{\\partial z_{x,y}^{(l)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now insert definition for $\\delta_{i,j}^{(l+1)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}} \\left[ z_{i,j}^{(l+1)} \\right].\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Write out $z_{i,j}^{(l+1)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}} \\left[ z_{i,j}^{(l+1)} \\right]\n",
    "    =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}}\n",
    "        \\left[ w^{(l+1)} \\circledast a_{i,j}^{(l)} + b^{(l+1)} \\right],\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Write out $a_{i,j}^{(l)}$ and use $\\frac{\\partial b^{(l+1)}}{\\partial z_{x,y}^{(l)}} = 0$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}}\n",
    "        \\left[ w^{(l+1)} \\circledast f~\\left(z_{i,j}^{(l)}\\right) \\right],\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Write out $w^{(l+1)} \\circledast f~\\left(z_{i,j}^{(l)}\\right)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}}\n",
    "        \\left[ \\sum \\limits_{u} ~ \\sum \\limits_{v}\n",
    "        \\left( w_{u, v}^{(l+1)} ~ f~\\left(z_{i - u, j - v}^{(l)}\\right) \\right) \\right],\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nonzero derivatives satisfy: $i - u = x \\wedge j - v = y$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}}\n",
    "        \\left[ \\sum \\limits_{u} ~ \\sum \\limits_{v}\n",
    "        \\left( w_{u, v}^{(l+1)} ~ f~\\left(z_{i - u, j - v}^{(l)}\\right) \\right) \\right]\n",
    "    \\\\\n",
    "    =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)}\n",
    "        w_{i - x, j - y}^{(l+1)} ~~ f~\\prime\\left(z_{x, y}^{(l)}\\right),\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Put constant derivative outside the sum:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    f~\\prime\\left(z_{x, y}^{(l)}\\right)\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)}\n",
    "        w_{i - x, j - y}^{(l+1)},\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Identify the double sum as a convolution of $\\delta_{x,y}^{(l+1)}$ over $w_{x,y}$, but flipped over both axes and remembering that $w$ is independent of $(x, y)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    f~\\prime\\left(z_{x, y}^{(l)}\\right)\n",
    "    \\left[ \n",
    "        \\delta_{x,y}^{(l+1)}\n",
    "        \\circledast\n",
    "        \\texttt{rot180}\\left( w_{x, y}^{(l+1)} \\right)\n",
    "    \\right]\n",
    "    =\n",
    "    f~\\prime\\left(z_{x, y}^{(l)}\\right)\n",
    "    \\left[ \n",
    "        \\delta_{x,y}^{(l+1)}\n",
    "        \\circledast\n",
    "        \\texttt{rot180}\\left( w^{(l+1)} \\right)\n",
    "    \\right]   \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error formula\n",
    "\n",
    "We now have a formula for $\\delta_{x,y}^{(l)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\delta_{x,y}^{(l)}\n",
    "    =\n",
    "    f~\\prime\\left(z_{x, y}^{(l)}\\right)\n",
    "    \\left[ \n",
    "        \\delta_{x,y}^{(l+1)}\n",
    "        \\circledast\n",
    "        \\texttt{rot180}\\left( w^{(l+1)} \\right)\n",
    "    \\right]    \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we can now derive the derivative of the loss function with respect to the weights.\n",
    "We begin by using the chain rule:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial C}{\\partial w_{i,j}^{(l)}}\n",
    "    =\n",
    "    \\sum \\limits_{x} \\sum \\limits_{y}\n",
    "    \\frac{\\partial C}{\\partial z_{x,y}^{(l)}}\n",
    "    \\frac{\\partial z_{x,y}^{(l)}}{\\partial w_{i,j}^{(l)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Replace the definition of $\\delta_{x,y}^{(l)}$ and write out $z_{x,y}^{(l)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{x} \\sum \\limits_{y}\n",
    "    \\delta_{x,y}^{(l)}\n",
    "    \\frac{\\partial}{\\partial w_{i,j}^{(l)}} \\left[\n",
    "        \\sum \\limits_{i} \\sum \\limits_{j}\n",
    "        w_{i,j}^{(l)} f~\\left(z_{x - i, y - j}^{(l - 1)} \\right) + b^{(l)}\n",
    "    \\right]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use the same manipulation as before in order to remove all zero terms caused by the derivative:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{x} \\sum \\limits_{y}\n",
    "    \\delta_{x,y}^{(l)} ~ f~\\left( z_{x - i, y - l}^{(l - 1)} \\right)\n",
    "    =\n",
    "    \\sum \\limits_{x} \\sum \\limits_{y}\n",
    "    \\delta_{x,y}^{(l)} ~ a_{x - i, y - j}^{(l - 1)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again we have a convolution of $\\delta_{x,y}^{(l)}$ over $\\texttt{rot180}\\left(a_{x, y}^{(l-1)}\\right)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial C}{\\partial w_{i,j}^{(l)}}\n",
    "    =\n",
    "    \\delta_{i,j}^{(l)} \\circledast \\texttt{rot180}\\left(a_{i, j}^{(l-1)}\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Repeating this logic with $\\frac{\\partial}{\\partial b^{(l)}}$ instead of $\\frac{\\partial}{\\partial w_{i,j^{(l)}}}$ we find that:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial C}{\\partial b^{(l)}} = \\sum \\limits_{x} \\sum \\limits_{y} \\delta_{x,y}^{(l)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Descent Update Equations\n",
    "\n",
    "With a constant learning rate of $\\alpha$ we update the kernel weight at location $(i,j)$ in layer $l$:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_{i,j}^{(l)} \\leftarrow w_{i,j}^{(l)} - \\alpha \\delta_{i,j}^{(l)} \\circledast \\texttt{rot180}\\left(a_{i, j}^{(l-1)}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "And likewise for the bias:\n",
    "\n",
    "\\begin{align*}\n",
    "    w^{(l)} \\leftarrow b^{(l)} - \\alpha \\sum \\limits_{x} \\sum \\limits_{y} \\delta_{x,y}^{(l)}\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
