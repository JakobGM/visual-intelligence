{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Classification and CNN\n",
    "\n",
    "_Authors: Martin Outzen Berild & Jakob Gerhard Martinussen_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://miro.medium.com/max/5856/1*Hz6t-tokG1niaUfmcysusw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Image classification before deep learning\n",
    "\n",
    "1. Feature extraction:\n",
    "    * **Naive:** Flatten $n \\times m$ RGB image into $\\mathcal{R}^{3nm}$ vector.\n",
    "    * **Manual feature extraction:** Custom feature extraction implemented by programmer based on domain-knowledge.\n",
    "2. Implement model based on features:\n",
    "    * **Explicit algorithm:** Pattern matching, etc.\n",
    "    * **Statistical model:** Support vector machine (SVM), etc.\n",
    "    \n",
    "Drawbacks:\n",
    "* Non-robust.\n",
    "* Time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep learning approach to image classification\n",
    "\n",
    "![](https://miro.medium.com/max/693/1*zUATaXMAmKof27rPyBRWsg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Convolutional Neural Networks Theory (ConvNets)\n",
    "\n",
    "_Keywords: receptive field, input, kernel, zero-padding, strides, output._\n",
    "\n",
    "![](https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors vs. Matrices\n",
    "\n",
    "**Key difference** - Inputs to CNNs are 2-D arrays instead of linearly indexed vectors.\n",
    "\n",
    "![](https://www.jeremyjordan.me/content/images/2017/07/Screen-Shot-2017-07-26-at-4.26.01-PM.png)\n",
    "_Source: [jeremyjordan.me](https://www.jeremyjordan.me)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Receptive Fields\n",
    "\n",
    "_Resource: [A Guide to Receptive Field Arithmetic for Convolutional Neural Networks](https://syncedreview.com/2017/05/11/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks/)_\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2340/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Number of filters to learn\n",
    "NUMBER_OF_FILTERS = 1\n",
    "\n",
    "# Size of receptive field\n",
    "KERNEL_SIZE = (3, 3)\n",
    "\n",
    "convolutional_layer = layers.Conv2D(\n",
    "    filters=NUMBER_OF_FILTERS,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "The process of moving the receptive field over the image, weighting each pixel in order to form new pixel values.\n",
    "\n",
    "This results in a so-called _\"feature map\"_, and can be thought of as a filtered image.\n",
    "\n",
    "<img src=\"https://machinethink.net/images/vggnet-convolutional-neural-network-iphone/ConvolutionKernel@2x.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "* A single bias value is also included.\n",
    "* The value is passed through an activation function as well.\n",
    "* The same kernel weights and bias value is used over the entire image (_weight/parameter sharing_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kernel\n",
    "\n",
    "_A set of weights defined over the receptive field._\n",
    "\n",
    "![](https://mlnotebook.github.io/img/CNN/convExample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/convolution_kernels_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](img/convolution_kernels_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stride\n",
    "\n",
    "_Step size of the receptive field as it moves over the input image._\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1185/1*L4T6IXRalWoseBncjRr4wQ@2x.gif\" width=\"49%\"> <img src=\"https://miro.medium.com/max/1082/1*4wZt9G7W7CchZO-5rVxl5g@2x.gif\" width=\"49%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding\n",
    "\n",
    "![](https://miro.medium.com/max/1595/1*W2D564Gkad9lj3_6t9I2PA@2x.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Convolution without padding and with stride of 1\n",
    "\n",
    "![](https://i0.wp.com/syncedreview.com/wp-content/uploads/2017/05/6.gif?resize=244%2C259&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolution with zero-padding and with stride of 1\n",
    "\n",
    "![](https://i2.wp.com/syncedreview.com/wp-content/uploads/2017/05/7.gif?resize=395%2C449&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolution with zero-padding and with stride of 2\n",
    "\n",
    "![](https://i2.wp.com/syncedreview.com/wp-content/uploads/2017/05/8.gif?resize=395%2C381&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pooling layer\n",
    "\n",
    "* A process which follows the convolution in CNNs.\n",
    "* Also known as _subsampling_.\n",
    "* Motivated by a model of the mammal visual cortex.\n",
    "\n",
    "> [...] a reduction in spatial resolution appears to be responsible for achieving translational invariance.\n",
    "\n",
    "* _Pooling_ is a way of modelling this reduction in dimensionality.\n",
    "* Benefits: makes the training computationally feasible and prevents overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Glossary\n",
    "\n",
    "* **pooled feature map** - the result of subsampling the convolutional feature map.\n",
    "* **pooling layer** - a collection of pooled feature maps.\n",
    "* **pooling neighborhood** - a subdivision of a feature map (usually 2x2) which is replaced by _one single_ value.\n",
    "* **adjacent pooling neighbourhood** - non-overlapping, side-by-side neighbourhoods.\n",
    "* **pooling method** - process of reducing neighbourhood down to one single value.\n",
    "    * **average pooling** - average of values within neighbourhood.\n",
    "    * **max pooling** - maximim value of neighbourhood.\n",
    "    * **$L_2$ pooling** - square root of sum of values in neighbourhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://vernlium.github.io/2018/10/15/coursera-deeplearning-ai-c4-week1/maxpool_animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "Classification is performed by a fully connected neural network (FCNN).\n",
    "\n",
    "* The FCNN's input comes from the last _pooled feature map_.\n",
    "* The 2D feature map is vectorized (read flattened) and fed into the input layer of the FCNN.\n",
    "\n",
    "![](https://csdl-images.computer.org/trans/tg/2017/01/figures/23tvcg01-liu-2598831-fig-2-source.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematical Formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "* Let $w \\in \\mathcal{R}^{\\texttt{KERNEL HEIGHT}~\\times~\\texttt{KERNEL WIDTH}}$ denote a given kernel.\n",
    "* Let $a_{x, y} \\in \\mathcal{R}$ denote image or pooled feature pixel value at index $(x, y)$\n",
    "    * $x = 1, ..., \\texttt{LAYER HEIGHT}$.\n",
    "    * $y = 1, ..., \\texttt{LAYER WIDTH}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The _convolution operator_, $\\circledast$, is then defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    w \\circledast a_{x, y} = \\sum_{i} \\sum_{j} w_{i, j} ~ a_{x - i, y - j}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Adding a _bias_, $b$, we define a _weighted output_, $z$, of the convolution as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    z_{x,y} = w \\circledast a_{x,y} + b\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forward pass\n",
    "\n",
    "* Let $l = 1, ..., L_{c}$ index the $L_c$ convolutional layers in the architecture. We then have:\n",
    "\n",
    "\\begin{equation*}\n",
    "    z_{x,y}^{(l)} = w^{(l)} \\circledast a_{x,y}^{(l - 1)} + b^{(l)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Let $f: \\mathcal{R} \\rightarrow \\mathcal{R}$ be the _activation function_ of choice. We then have:\n",
    "\n",
    "\\begin{equation*}\n",
    "    a_{x,y}^{(l)} = f ~ \\left(z_{x,y}^{(l)}\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $a_{x,y}^{(0)} = \\{\\text{values of pixels in the original input image}\\}$.\n",
    "* $a_{x,y}^{(L_c)} = \\{\\text{values of pooled features in last layer of the CNN}\\}$.\n",
    "* NB! The notation is a bit sloppy here, as $a_{x,y}^{(l)}$ is the result of a downsampling (pooling) in addition to an activation $f(...)$. Consider $f(...)$ to be the activation function and downsampler function from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://miro.medium.com/max/1522/1*32zCSTBi3giSApz1oQV-zA.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Let $C$ denote the cost function.\n",
    "* The error at position $(x, y)$ in the _pooled layer_ number $l$ is then defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\delta_{x,y}^{(l)} = \\frac{\\partial C}{\\partial z_{x,y}^{(l)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $\\mathcal{I}$ and $\\mathcal{J}$ be index sets which contain all the indeces of $i$ and $j$ which are involved in the calculation of $z_{x, y}$. Using the chain rule we can relate $\\delta_{x,y}^{(l)}$ to $\\delta_{x,y}^{(l+1)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\delta_{x,y}^{(l)}\n",
    "    =\n",
    "    \\frac{\\partial C}{\\partial z_{x,y}^{(l)}}\n",
    "    =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\frac{\\partial C}{\\partial z_{i,j}^{(l+1)}} \\frac{\\partial z_{i,j}^{(l+1)}}{\\partial z_{x,y}^{(l)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now insert definition for $\\delta_{i,j}^{(l+1)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}} \\left[ z_{i,j}^{(l+1)} \\right].\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Write out $z_{i,j}^{(l+1)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}} \\left[ z_{i,j}^{(l+1)} \\right]\n",
    "    =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}}\n",
    "        \\left[ w^{(l+1)} \\circledast a_{i,j}^{(l)} + b^{(l+1)} \\right],\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Write out $a_{i,j}^{(l)}$ and use $\\frac{\\partial b^{(l+1)}}{\\partial z_{x,y}^{(l)}} = 0$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}}\n",
    "        \\left[ w^{(l+1)} \\circledast f~\\left(z_{i,j}^{(l)}\\right) \\right],\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Write out $w^{(l+1)} \\circledast f~\\left(z_{i,j}^{(l)}\\right)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}}\n",
    "        \\left[ \\sum \\limits_{u} ~ \\sum \\limits_{v}\n",
    "        \\left( w_{u, v}^{(l+1)} ~ f~\\left(z_{i - u, j - v}^{(l)}\\right) \\right) \\right],\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nonzero derivatives satisfy: $i - u = x \\wedge j - v = y$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)} \\frac{\\partial}{\\partial z_{x,y}^{(l)}}\n",
    "        \\left[ \\sum \\limits_{u} ~ \\sum \\limits_{v}\n",
    "        \\left( w_{u, v}^{(l+1)} ~ f~\\left(z_{i - u, j - v}^{(l)}\\right) \\right) \\right]\n",
    "    \\\\\n",
    "    =\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)}\n",
    "        w_{i - x, j - y}^{(l+1)} ~~ f~\\prime\\left(z_{x, y}^{(l)}\\right),\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Put constant derivative outside the sum:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    f~\\prime\\left(z_{x, y}^{(l)}\\right)\n",
    "    \\sum \\limits_{i \\in \\mathcal{I}} ~ \\sum \\limits_{j \\in \\mathcal{J}} ~ \n",
    "    \\delta_{i,j}^{(l+1)}\n",
    "        w_{i - x, j - y}^{(l+1)},\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Identify the double sum as a convolution of $\\delta_{x,y}^{(l+1)}$ over $w_{x,y}$, but flipped over both axes and remembering that $w$ is independent of $(x, y)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    f~\\prime\\left(z_{x, y}^{(l)}\\right)\n",
    "    \\left[ \n",
    "        \\delta_{x,y}^{(l+1)}\n",
    "        \\circledast\n",
    "        \\texttt{rot180}\\left( w_{x, y}^{(l+1)} \\right)\n",
    "    \\right]\n",
    "    =\n",
    "    f~\\prime\\left(z_{x, y}^{(l)}\\right)\n",
    "    \\left[ \n",
    "        \\delta_{x,y}^{(l+1)}\n",
    "        \\circledast\n",
    "        \\texttt{rot180}\\left( w^{(l+1)} \\right)\n",
    "    \\right]   \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error formula\n",
    "\n",
    "We now have a formula for $\\delta_{x,y}^{(l)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\delta_{x,y}^{(l)}\n",
    "    =\n",
    "    f~\\prime\\left(z_{x, y}^{(l)}\\right)\n",
    "    \\left[ \n",
    "        \\delta_{x,y}^{(l+1)}\n",
    "        \\circledast\n",
    "        \\texttt{rot180}\\left( w^{(l+1)} \\right)\n",
    "    \\right]    \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we can now derive the derivative of the loss function with respect to the weights.\n",
    "We begin by using the chain rule:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial C}{\\partial w_{i,j}^{(l)}}\n",
    "    =\n",
    "    \\sum \\limits_{x} \\sum \\limits_{y}\n",
    "    \\frac{\\partial C}{\\partial z_{x,y}^{(l)}}\n",
    "    \\frac{\\partial z_{x,y}^{(l)}}{\\partial w_{i,j}^{(l)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Replace the definition of $\\delta_{x,y}^{(l)}$ and write out $z_{x,y}^{(l)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{x} \\sum \\limits_{y}\n",
    "    \\delta_{x,y}^{(l)}\n",
    "    \\frac{\\partial}{\\partial w_{i,j}^{(l)}} \\left[\n",
    "        \\sum \\limits_{i} \\sum \\limits_{j}\n",
    "        w_{i,j}^{(l)} f~\\left(z_{x - i, y - j}^{(l - 1)} \\right) + b^{(l)}\n",
    "    \\right]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use the same manipulation as before in order to remove all zero terms caused by the derivative:\n",
    "\n",
    "\\begin{equation*}\n",
    "    ... =\n",
    "    \\sum \\limits_{x} \\sum \\limits_{y}\n",
    "    \\delta_{x,y}^{(l)} ~ f~\\left( z_{x - i, y - l}^{(l - 1)} \\right)\n",
    "    =\n",
    "    \\sum \\limits_{x} \\sum \\limits_{y}\n",
    "    \\delta_{x,y}^{(l)} ~ a_{x - i, y - j}^{(l - 1)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Again we have a convolution of $\\delta_{x,y}^{(l)}$ over $\\texttt{rot180}\\left(a_{x, y}^{(l-1)}\\right)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial C}{\\partial w_{i,j}^{(l)}}\n",
    "    =\n",
    "    \\delta_{i,j}^{(l)} \\circledast \\texttt{rot180}\\left(a_{i, j}^{(l-1)}\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Repeating this logic with $\\frac{\\partial}{\\partial b^{(l)}}$ instead of $\\frac{\\partial}{\\partial w_{i,j^{(l)}}}$ we find that:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial C}{\\partial b^{(l)}} = \\sum \\limits_{x} \\sum \\limits_{y} \\delta_{x,y}^{(l)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Descent Update Equations\n",
    "\n",
    "With a constant learning rate of $\\alpha$ we update the kernel weight at location $(i,j)$ in layer $l$:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_{i,j}^{(l)} \\leftarrow w_{i,j}^{(l)} - \\alpha \\delta_{i,j}^{(l)} \\circledast \\texttt{rot180}\\left(a_{i, j}^{(l-1)}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "And likewise for the bias:\n",
    "\n",
    "\\begin{align*}\n",
    "    w^{(l)} \\leftarrow b^{(l)} - \\alpha \\sum \\limits_{x} \\sum \\limits_{y} \\delta_{x,y}^{(l)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN and the MNIST dataset (Numerical Example)\n",
    "\n",
    "Following the [Convolutional Neural Networks Tensorflow 2.0 tutorial](https://www.tensorflow.org/beta/tutorials/images/intro_to_cnns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Enable GPU support\n",
    "\n",
    "In order to enable GPU support on Tensorflow v2.0 in Jupyter Lab, follow [this guide](https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-464910864)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Dense layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_images, test_labels, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"val_accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
