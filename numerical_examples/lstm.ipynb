{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)\n",
    "\n",
    "LSTM was first introduced by <a href=\"https://www.bioinf.jku.at/publications/older/2604.pdf\">Hochreiter & Schmidhuber (1997)</a>, solving the issues of long-term dependencies that previous reccurent neural networks (RNN) had.\n",
    "We will refer to the <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">explanatory blog post</a> for a thorough  introduction to LSTM networks.\n",
    "A RNN consist of a chain of cells that are repeated throughout the network.\n",
    "For a standard RNN this cell is simple and consist of only one layer with two inputs; the previous hidden state (output from the previous cell), and the new event (new data that corresponds to this timestep).\n",
    "A visualization of the RNN architecture can be viewed below, where a chain of three cells is depicted.\n",
    "However, this chain can be chosen to be much longer, a hyperparameter that can be tuned accordingly.\n",
    "\n",
    "<img src=\"https://i.imgur.com/ZYHXrLB.png\" style=\"width:50%;display: block;margin-left: auto;margin-right: auto;\">\n",
    "\n",
    "The LSTM network has much more complex cells than a simple RNN, and it includes multiple hidden layers.\n",
    "It, however, keeps the chain-like structure of repeating cells. \n",
    "In the figure below the different layers and combinatory operations are depicted. \n",
    "From the left the different layers are; *forget gate layer*, *input gate layer*, *learning layer*, and *ouput gate layer*. \n",
    "These will be further explained in the following sections. \n",
    "We will also present `numpy` implementations of the different aspects of a LSTM network as they are introduced.\n",
    "\n",
    "<img src=\"https://i.imgur.com/F42ltRd.png\" style=\"width:50%;display: block;margin-left: auto;margin-right: auto;\">\n",
    "\n",
    "To start we must first implement the activation functions used in LSTM.\n",
    "The first one being the sigmoid function, which will map the input values into the domain $(0, 1)$ (squeeze), and is defined by:\n",
    "$$\n",
    " \\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{e^x+1}.\n",
    "$$\n",
    "Our implementation of the sigmoid function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lstm\n",
    "%psource lstm.sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second activation function is the *tanh* function, which will squeeze values between minus one and one. The function is \n",
    "$$\n",
    " \\mathrm{tanh}(x) = \\frac{\\mathrm{sinh}(x)}{\\mathrm{cosh}(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{e^{2x} - 1}{e^{2x} + 1},\n",
    "$$\n",
    "and our implementation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mnumerator\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource lstm.tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, in order to implement an LSTM cell in numpy, as presented in the figure above, we will create a `Parameters` class containing the entire parametrization of the LSTM network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mclass\u001b[0m \u001b[0mParameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Weights and biases for the sigmoid \"f-function\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_forget_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_forget_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_forget_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_forget_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Weights and biases for the sigmoid \"i-function\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_update_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_update_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_update_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_update_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Weights and biases for the tanh \"C-bar-function\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_candidate_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_candidate_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_candidate_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_candidate_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Weights and biases for the \"o-function\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_output_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_output_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource lstm.Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now instantiate this class.\n",
    "For our dummy example we will set the input size $\\mathrm{dim}(x_t) = (3,1)$ and the hidden size $\\mathrm{dim}(h_t) = (3,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_SIZE, HIDDEN_SIZE = 3, 3\n",
    "parameters = lstm.Parameters(event_size=EVENT_SIZE, hidden_size=HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell state\n",
    "\n",
    "In a simple RNN the hidden state, $h$, carries the information about what the network has seen in the past and will give this information to the subsequent cell.\n",
    "The problem with having the hidden state carry this historical information is that each historical event is included in the loss gradient as a multiplicative factor.\n",
    "This will in manny cases yeild exploding or vanishing gradients.\n",
    "LSTMs introduces the *Cell State* to combat this issue.\n",
    "It works by contributing to the loss function in an additive manner instead, thereby yeilding a constant gradient between current and previous time states.\n",
    "The cell state is the \"conveyor belt\" running straight through the network and is displayed in the figure below. \n",
    "\n",
    "<img src=\"https://i.imgur.com/hyUkhnU.png\" style=\"width:50%;display: block;margin-left: auto;margin-right: auto;\">\n",
    "\n",
    "Now the hidden state, $h$, carries the information seen in the previous state, and the cell state, $C$, carries the historical data or the long-term memory.\n",
    "The cell state has a multiplicative intercation with the forget gate layer, that works exactly like the name, it helps the network forget irrelevant information in the long-term memory.\n",
    "It also has an additive interaction with the input/learning gate layer, that will update the long-term memory with new information to be kept for later.\n",
    "Next, the cell state contributes with the calculation of the update of the hidden state in the cell.\n",
    "Finally, the cell state is sent straight through to the next cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forget Gate Layer\n",
    "\n",
    "The *forget gate layer* is the first layer within the LSTM cell; it is furthest to the left in the cell diagram and highlighted in the visualization below.\n",
    "This layer is responsible for forgetting irrelevant parts of the long-term memory kept in the cell state.\n",
    "It uses the new information in $x_t$ and the information from the previous cell $h_{t-1}$ to determine this. \n",
    "\n",
    "<img src=\"https://i.imgur.com/3BwMSrF.png\" style=\"width:50%;display: block;margin-left: auto;margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_{hf}$ are the weights of the different units in the hidden state, $h_t$, and $W_{xf}$ are the weights of the units in the new event $x_t$.\n",
    "$B_{hf}$ and $B_{xf}$ is the bias of the two different branches in the forget gate layer.\n",
    "The output of these branches are added together and passed through the sigmoid activation function.\n",
    "These operation can be summarized in the following equation\n",
    "$$\n",
    "f_t = \\sigma(W_{hf}\\cdot h_{t-1} + W_{xf}\\cdot x_t + B_f).\n",
    "$$\n",
    "The output of this activation is between zero and one, and is further multiplied with the cell state.\n",
    "Thereby, the cell state \"forgets\" irrelevant historical information. \n",
    "\n",
    "The forget gate is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mforget_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_cell_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Forget gate deciding how much of the previous cell state to keep.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforget_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_forget_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_forget_bias\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforget_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_forget_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_forget_bias\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Values between zero and one indicating how much to forget\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforgetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforget_hidden\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mforget_event\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Return the state that should be kept\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkept_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforgetter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_cell_state\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mkept_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource lstm.forget_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we have an existing hidden state of $\\vec{h} = [0, 0, 10]$, a previous cell state of $\\vec{C} = [1, 1, 1]$, and a new event $\\vec{x} = [10, 0, 0]$.\n",
    "What happens in the forget gate with the current parametrization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hidden_state = np.array([0, 0, 10])\n",
    "prev_cell_state = np.array([1, 1, 1])\n",
    "event = np.array([10, 0, 0])\n",
    "lstm.forget_gate(event=event, hidden_state=hidden_state, parameters=parameters, prev_cell_state=prev_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have zero-weights, and zero biases, the new cell state has become $[0.5, 0.5, 0.5]$ since $\\mathrm{sigmoid}(1) = 0.5$.\n",
    "Let's change the weight matrices to identity matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999546, 0.5      , 0.9999546])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.event_forget_weights = np.eye(HIDDEN_SIZE, EVENT_SIZE)\n",
    "parameters.hidden_forget_weights = np.eye(HIDDEN_SIZE, EVENT_SIZE)\n",
    "lstm.forget_gate(event=event, hidden_state=hidden_state, parameters=parameters, prev_cell_state=prev_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing both weight matrices to the identity matrices, the previous hidden state and the new event has influenced the new cell state.\n",
    "We can also let the new event influence the cell state, and the hidden state be completely ignored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9999546, 0.5      , 0.5      ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.event_forget_weights = np.eye(HIDDEN_SIZE, EVENT_SIZE)\n",
    "parameters.hidden_forget_weights = np.zeros((HIDDEN_SIZE, EVENT_SIZE))\n",
    "lstm.forget_gate(event=event, hidden_state=hidden_state, parameters=parameters, prev_cell_state=prev_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first index has been changed from $0.5$ to $\\approx 1$, while the third index is still $0.5$ as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/Learning gate Layer\n",
    "\n",
    "The *input gate layer* is the second layer from the left in the cell diagram.\n",
    "It is interacting with the *learning* layer, which is the third layer.\n",
    "The figure below highlights both these layers and the operations within.\n",
    "The function of these two layers is to update the cell state with new relevant information for the long-term memory.\n",
    "First, the input gate decides which information from the previous cell state and the new event information is relevant (candidates).\n",
    "At the same time the learning gate will weigh the different features.\n",
    "Secondly, the output of these layers are multiplied together and added to the cell state.\n",
    "This means that the update either \"ignores\" the weighted features or \"remembers\" them, somewhat simplified.\n",
    "\n",
    "<img src=\"https://i.imgur.com/2qCZtNO.png\" style=\"width:50%;display: block;margin-left: auto;margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input gate layer uses the sigmoid activation function. $h_{t-1}$ and $x_t$ is again weighted by individual model matricies; $W_{hi}$ and $W_{xi}$.\n",
    "The bias of this layer can be combined into a single term $B_i$, and the operation can therefore be expressed as:\n",
    "$$\n",
    "i_t = \\sigma(W_{hi}\\cdot h_{t-1} W_{xi}\\cdot x_t + B_i).\n",
    "$$\n",
    "The learning layer uses the tanh activation function.\n",
    "The features $h_{t-1}$ and $x_t$ are weighted by their respective model matrices; $W_{hi}$ and $W_{xi}$, then they are added together and passed through the activation function.\n",
    "These operations are summarized in the following equation:\n",
    "$$\n",
    "l_t = \\mathrm{tanh}(W_{hl}\\cdot h_{t-1} + W_{xl}\\cdot x_t + B_t).\n",
    "$$\n",
    "$B_t$ is again the combined bias in this layer. \n",
    "\n",
    "Lastly, the outputs of these layers are multiplied together to form the update of the cell state.\n",
    "This update is then added to the cell state, yielding the new cell state $C_t$.\n",
    "This summarized in the following expression:\n",
    "$$\n",
    " C_t = C_{t-1}\\times f_t + i_t \\times l_t. \n",
    "$$\n",
    "\n",
    "Our implementation of the input gate and learning gate is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0minput_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Input gate deciding how to update the cell state.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# We have certain candidates from the new event and the hidden state\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# we would like to update the cell state with\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhidden_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_candidate_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_candidate_bias\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mevent_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_candidate_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_candidate_bias\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# We must also determine how much to weigh these updates\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mevent_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_update_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_update_bias\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhidden_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_update_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_update_bias\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Finally returning the update\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_update\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_candidates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource lstm.input_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to evaluate this gate with the default $0$ weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.input_gate(event=event, hidden_state=hidden_state, parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we get $\\vec{0}$ as the output from the input gate, due to the $\\tanh$ activation function being zero for zero input.\n",
    "Let's see what happens if we set the weight matrix for the hidden state to the identity matrix, but leaving the event weights as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.       , 0.       , 0.9999546])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.hidden_candidate_weights = np.eye(HIDDEN_SIZE, EVENT_SIZE)\n",
    "parameters.hidden_update_weights = np.eye(HIDDEN_SIZE, EVENT_SIZE)\n",
    "lstm.input_gate(event=event, hidden_state=hidden_state, parameters=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting update weighs the hidden state and ignores the new event data completely, as expected.\n",
    "The new cell state is the sum of the forget gate output and the input gate output, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforget_gate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_gate_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    New cell state, a combination of the partially forgotten cell state\u001b[0m\n",
       "\u001b[0;34m    and the newly proposed state.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mforget_gate_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_gate_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource lstm.cell_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Gate Layer\n",
    "\n",
    "The last layer in the LSTM cell is responsible for the output of the network, referred to as the *output gate layer*.\n",
    "The output is also the updated hidden state of the cell, $h_{t-1}$, which will be used in the subsequent cell.\n",
    "This layer operates by chosing the information relevant from the updated cell state to the current hidden state.\n",
    "The layer is highlighted in the following figure.\n",
    "\n",
    "<img src=\"https://i.imgur.com/YGxPsJY.png\" style=\"width:50%;display: block;margin-left: auto;margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially the cell state is applied to a tanh function to squeeze the different features between minus one and one.\n",
    "It is important to note that the cell state hasn't changed in this layer, but rather used as a set of features to calculate the new hidden state.\n",
    "The current hidden state $h_{t-1}$ and the event $x_t$ is then weighted according to the model matricies of the output layer; $W_{ho}$ and $W_{xo}$, and added together with a bias $B_o$.\n",
    "Then the sigmoid function is applied to these weighted features.\n",
    "The operations is given by the equation\n",
    "$$\n",
    " o_t = \\sigma(W_{ho}\\cdot h_{t-1} + W_{xo}\\cdot x_t + B_o).\n",
    "$$\n",
    "The output of this activation will choose the relevant information in the cell state by multiplying $o_t$ with the tanh of the cell state.\n",
    "Thereby, $o_t$ will weigh the cell state information by their relevance to the new hidden state, and is given by the equation\n",
    "$$\n",
    " h_t = o_t\\times \\mathrm{tanh}(C_t).\n",
    "$$\n",
    "The output of the LSTM is now calculated, and the new hidden state $h_t$ is sent into the next cell.\n",
    "For the given LSTM network we have implemented, the event input size is $x \\times 1$, and the hidden state is of size $h \\times 1$.\n",
    "This implies that the output of the LSTM cell becomes $h \\times 1$, which is not necessarily optimal for the given problem.\n",
    "It is therefore customary to pass the output into a final, fully connected neural network that will further optimize the output for the desired task.\n",
    "We have opted to not implement this last part, since it is not a core, new concept of LSTM cells.\n",
    "\n",
    "Our implementation of the output gate layer is given by the follwing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0moutput_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Decide what to output from the LSTM cell.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhidden_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_output_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_output_bias\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mevent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_output_weights\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_output_bias\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource lstm.output_gate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
