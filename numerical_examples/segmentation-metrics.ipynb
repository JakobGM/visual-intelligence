{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Metrics\n",
    "\n",
    "In this numerical example, we will look at different metrics which can be used for image segmentation problems.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Let's start by importing the python modules which we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Example\n",
    "\n",
    "We will begin by defining a dummy data point.\n",
    "Consider a binary `0`/`1` pixel-wise segmentation problem, with image dimension $256 \\times 256$.\n",
    "\n",
    "The ground truth will be a rectangle of size $120 \\times 80$, while the \"predicted\" mask will be of the same size, only offset by $(-30, -30)$.\n",
    "Let's plot this scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MASK_WIDTH, MASK_HEIGHT = 256, 256\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "y_true = np.zeros(shape=(MASK_HEIGHT, MASK_WIDTH, NUM_CLASSES), dtype=\"uint8\")\n",
    "y_pred = np.zeros(shape=(MASK_HEIGHT, MASK_WIDTH, NUM_CLASSES), dtype=\"uint8\")\n",
    "\n",
    "y_true[80:200, 120:200] = 1\n",
    "y_pred[50:170, 90:170] = 1\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "ax1.set_title(\"Ground Truth\")\n",
    "ax2.set_title(\"Predicted Mask\")\n",
    "\n",
    "ax1.imshow(np.squeeze(y_true), cmap=\"binary\");\n",
    "ax2.imshow(np.squeeze(y_pred), cmap=\"binary\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pixels have been correctly classified, while others have not, both by predicting `0`, when really `1` is the case, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False Positives/Negatives\n",
    "\n",
    "For ground truths in classification problems, we have the following definitions:\n",
    "\n",
    "* **Condition Positive (P)** - Number of class $A$ in the data.\n",
    "* **Condition Negative (N)** - Number of class $A^C$ in the data.\n",
    "\n",
    "Where $A^C$ denotes the class complement of $A$, i.e. all other classes besides class $A$.\n",
    "In our binary classification problem $A$ denotes `1` and $A^C$ denotes `0`.\n",
    "\n",
    "The conditions are easily calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mconditions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpositives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpositives\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpositives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource metrics.conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P = 9600\n",
      "N = 55936\n"
     ]
    }
   ],
   "source": [
    "P, N = metrics.conditions(y_true=y_true)\n",
    "print(f\"P = {P}\")\n",
    "print(f\"N = {N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four related concepts need also be defined:\n",
    "\n",
    "* **True Positive (TP)** - Class $A$ correctly predicted as $A$ (correctly identified).\n",
    "* **True Negative (TN)** - Class $A^C$ correctly predicted as $A^C$ (correctly rejected).\n",
    "\n",
    "* **False Positive (FP)** - $A^C$ wrongly predicted as $A$ (incorrectly identified).\n",
    "* **False Negative (FN)** - $A$ wrongly predicted as $A^C$ (incorrectly rejected).\n",
    "\n",
    "False positves (FP) are often knows as _type I errors_ in statistics, and false negatives (FN) as _Type II errors_.\n",
    "The greater the values of TP and TN, the better, and the smaller the values of FP and FN, the better. We will collectively refer to these four values as _confusions_, for a lack of a better name.\n",
    "\n",
    "These metrics will also be implemented now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mconfusions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource metrics.confusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* **TP** = 4500"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* **TN** = 50836"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* **FP** = 5100"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* **FN** = 5100"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusions = metrics.confusions(y_true=y_true, y_pred=y_pred)\n",
    "for name, metric in zip(\"TP TN FP FN\".split(), confusions):\n",
    "    display(Markdown(f\"* **{name}** = {metric}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pixel-wise segmentation tasks, every single pixel in the predicted mask will be labeled as one of these four categories (FP, TN, FP, FN).\n",
    "\n",
    "We can therefore plot these labels in order to get a more intuitive picture of how these labels are distributed in practice.\n",
    "The following function does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mplot_conditions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcutoff\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpredicted_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListedColormap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m[\u001b[0m\u001b[0;34m'#001F3F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#DDDDDD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#2ECC40'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#FF4136'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoundaryNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_yaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Add TP/TN/FP/FN legend to plot\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlegend_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mPatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#001F3F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mPatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#DDDDDD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mPatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#2ECC40'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mPatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#FF4136'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mhandles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlegend_elements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lower center\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mncol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbbox_to_anchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.125\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mframeon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mhandlelength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mhandleheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource metrics.plot_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this on our dummy problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.plot_conditions(y_pred=y_pred, y_true=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more more numerical, and less spatial, representation of the prevalence of each label type is the so-called _confusion matrix_.\n",
    "It shows the values of TP, TN, FP, and FN in tabular form.\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Positive Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Negative Prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Condition Positive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Condition Negative\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource metrics.confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition Positive</th>\n",
       "      <th>Condition Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Positive Prediction</td>\n",
       "      <td>4500</td>\n",
       "      <td>5100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Negative Prediction</td>\n",
       "      <td>5100</td>\n",
       "      <td>50836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Condition Positive  Condition Negative\n",
       "Positive Prediction                4500                5100\n",
       "Negative Prediction                5100               50836"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_pred=y_pred, y_true=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity and Specificity\n",
    "\n",
    "One of the issues with the confusion metrics, is that it is not _sample size invariant_.\n",
    "With other words, it is not straight forward to compare two confusion matrices when the metrics have been calculated of two datasets of different size. The concepts of _sensitivity_ and _specificity_ helps with this comparison problem:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\textbf{sensitivity}\n",
    "    &=\n",
    "    \\frac{\\text{number of true positives}}{\\text{number of true positives + number of false negatives}}\n",
    "    =\n",
    "    \\frac{TP}{TP + FN}\n",
    "    =\n",
    "    \\frac{TP}{P}\n",
    "    \\\\\n",
    "    \\textbf{specificity}\n",
    "    &=\n",
    "    \\frac{\\text{number of true negatives}}{\\text{number of true negatives + number of false positives}}\n",
    "    =\n",
    "    \\frac{TN}{TN + FP}\n",
    "    =\n",
    "    \\frac{TN}{N}\n",
    "\\end{align*}\n",
    "\n",
    "The _sensitivity_ is therefore a measure of how good the given model prediction was able to identify positives as a relative, frational value.\n",
    "Likewise, the _specificity_ is a measure of how good the given model prediction was able to identify negatives as a relative, fractional value.\n",
    "\n",
    "We will now implement these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0msensitivity_and_specificity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msensitivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mspecificity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msensitivity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecificity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource metrics.sensitivity_and_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "* **sensitivity** = 46.88%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "* **specificity** = 90.88%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sensitivity, specificity = metrics.sensitivity_and_specificity(\n",
    "    y_pred=y_pred,\n",
    "    y_true=y_true,\n",
    ")\n",
    "display(Markdown(f\"* **sensitivity** = {100 * sensitivity:.2f}%\"))\n",
    "display(Markdown(f\"* **specificity** = {100 * specificity:.2f}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that for the given dummy problem, the prediction is \"much better\" at identifying negatives compared to positives.\n",
    "This comes as no surprise, given the larger amount of negatives and how the dummy problem is constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel Accuracy\n",
    "\n",
    "A more naive metric for semantic segmentation problems is the _pixel accuracy_.\n",
    "This metric simply reports the percentage of pixels that were correctly classified.\n",
    "More formally, it can be defined as:\n",
    "\n",
    "$$\n",
    "    \\textbf{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{TP + TN}{P + N}\n",
    "$$\n",
    "\n",
    "This metric will now be implemented and evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource metrics.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**accuracy** = 84.44%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = metrics.accuracy(y_true=y_true, y_pred=y_pred)\n",
    "display(Markdown(f\"**accuracy** = {100 * accuracy:.2f}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with pixel-wise accuracy metrics is that it does not account for class inbalances.\n",
    "Consider a problem where 95% of all pixels are considered to be of class `0`, and the remaining 5% of class `1`.\n",
    "If we construct a model which predicts `0.0` regardless of the fiture inputs provided to the model, the model will achieve a 95% accuracy score.\n",
    "Generally, a model which predicts always predicts `0` on a test set with class balance $\\alpha / (1 - \\alpha)$ will achieve a pixel-wise accuracy score of $\\alpha$.\n",
    "\n",
    "This makes pixel-wise accuracy scores basically useless when you do not know the class balance of the underlying dataset and the actual predictions across the classes.\n",
    "This is why it is often replaced by other metrics which takes inbalances and model confidence into account.\n",
    "\n",
    "Write about other metrics as explained [here](https://www.jeremyjordan.me/evaluating-image-segmentation-models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Intersection over Union (IoU) Metric\n",
    "\n",
    "A more commonly used metric for segmentation metrics is the intersection over union metric.\n",
    "It is defined as follows:\n",
    "\n",
    "$$\n",
    "    \\mathrm{IoU}\n",
    "    =\n",
    "    \\frac{\\mathrm{target} \\cap \\mathrm{prediction}}{\\mathrm{target} \\cup \\mathrm{prediction}}\n",
    "    =\n",
    "    \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}}\n",
    "$$\n",
    "\n",
    "The implementation in form of code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mdef\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource metrics.iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, we will evaluate this metric on our dummy problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**IoU** = 0.30612244897959184"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iou = metrics.iou(y_true=y_true, y_pred=y_pred)\n",
    "display(Markdown(f\"**IoU** = {iou}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this metric captures the fact that our prediction is quite sub-optimal.\n",
    "The IoU metric's range is $[0, 1]$ inclusive, where `0` indicates that no positive case has been correctly classified at all, and `1` indicates perfect overlap between predicted mask and ground truth mask.\n",
    "\n",
    "This metric is much more balanced.\n",
    "In the case where there are no positive conditions in the ground truth, $\\mathrm{TP} \\equiv 0$ and $\\mathrm{IoU} \\equiv 0$, by definition.\n",
    "So $\\mathrm{IoU}$ will become zero for the given positive class _even_ if the predicted mask is perfectly consistent with the ground truth.\n",
    "In such cases it is common to define $\\mathrm{IoU} = 1$ if $\\mathrm{P} = 0$.\n",
    "The drawback of this approach is that the metric will not capture false positives.\n",
    "\n",
    "In the case of multiple classes, you calculate one IoU metric for each class and average across the classes. This is called mean IoU, and can be used to remedy the problem mentioned in the previous paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
